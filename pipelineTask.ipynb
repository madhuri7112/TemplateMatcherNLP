{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stat_parser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a936aeef91ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstat_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stat_parser'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from stat_parser import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, id, title, content, link):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.link = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchArticles():\n",
    "    files = [\"xac\"]\n",
    "    articles = {}\n",
    "    for filename in files:\n",
    "        with open(filename) as data_file:\n",
    "            fileData = data_file.readlines()\n",
    "        for articleData in fileData:\n",
    "            articleComponents = articleData.split(\"\\t\")\n",
    "            articles[articleComponents[0]] = (Article(articleComponents[0], articleComponents[1], articleComponents[2], articleComponents[3]))\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(records):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() \n",
    "    for record in records:\n",
    "        sentences = nltk.sent_tokenize(record)\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            print(sentence)\n",
    "            print(nltk.pos_tag(sentence))\n",
    "            for word in words:\n",
    "                print(\"Word: \", word)\n",
    "                print(\"Lemma: \", wordnet_lemmatizer.lemmatize(word))\n",
    "                for synset in wordnet.synsets(word):\n",
    "                    print (synset, synset.hypernyms())\n",
    "                    print (synset, synset.hyponyms())\n",
    "                    print (synset, synset.part_meronyms())\n",
    "                    print (synset, synset.substance_meronyms())\n",
    "                    print (synset, synset.part_holonyms())\n",
    "                    print (synset, synset.substance_holonyms())\n",
    "          \n",
    "    print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Sentences and words\n",
    "articles = fetchArticles()\n",
    "titles = [article.title for article in articles.values()]\n",
    "\n",
    "pipeline(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "def isARelationship(word1, word2, word1ContainingSentence):\n",
    "    sent = nltk.word_tokenize(word1ContainingSentence)\n",
    "    print(lesk(sent, word1, 'n'))\n",
    "    \n",
    "isARelationship(\"coach\", \"ff\", \"He is a great coach for the players\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd import disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TYPE_PERSON_SYNSET = \"person.n.01\"\n",
    "TYPE_ANIMAL_SYNET = \"animal.n.01\"\n",
    "MAX_ITER_IS_A = 5\n",
    "# return true if \"word1\" is-a \"word2\"\n",
    "# Eg: returns true if word1 = cat, word2 = animal because cat is-a animal\n",
    "def isARelationship(word1, word2, word1ContainingSentence=\"\"):\n",
    "\n",
    "\tdef isAUtil(word1Synset, word2Synset, numIter):\n",
    "\t\tif numIter ==  MAX_ITER_IS_A:\n",
    "\t\t\treturn false\n",
    "\t\thypernyms = word1Synset.hypernyms()\n",
    "\t\tif word2Synset in hypernyms:\n",
    "\t\t\treturn True\n",
    "\t\n",
    "\t\tfor hypernym in hypernyms:\n",
    "\t\t\tif isAUtil(hypernym, word2Synset, ++numIter):\n",
    "\t\t\t\treturn True\n",
    "\t\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "\tword1Synsets = wordnet.synsets(word1)\n",
    "\tword2Synset = wordnet.synset(word2)\n",
    "\n",
    "\tif word2Synset in word1Synsets:\n",
    "\t\treturn True\n",
    "\n",
    "\tfor syn in word1Synsets:\n",
    "\t\tif isAUtil(syn, word2Synset, 0):\n",
    "\t\t\treturn True\n",
    "\n",
    "\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isARelationship(\"computer\", TYPE_ANIMAL_SYNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
